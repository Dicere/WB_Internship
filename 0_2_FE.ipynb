{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Q7Ap_Gq2TMqKs_rTwCINUCd0Y7vXTnrh",
      "authorship_tag": "ABX9TyOyjU4N/ZkP8xgpsZHwK3OP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dicere/WB_Internship/blob/main/0_2_FE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXhACCW3Xdih"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install pymorphy2\n",
        "!pip string\n",
        "!pip install emoji\n",
        "!pip install catboost\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install pyspellchecker\n",
        "!pip install deeppavlov\n",
        "!pip install transformers\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import pymorphy2\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import emoji\n",
        "import collections\n",
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from deeppavlov import build_model, configs\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import BertTokenizerFast\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_XM5iUeXoU4",
        "outputId": "88649c43-0324-40a0-938e-f2a1e8b1ca88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from catboost import CatBoostClassifier\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "dbWZK5pJXp6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment-rusentiment')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment-rusentiment', return_dict=True)\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "tlPtjZ2SdYaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/WB_ÑÑ‚Ğ°Ğ¶Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°/wb_school_task_2.csv.gzip',compression='gzip')\n"
      ],
      "metadata": {
        "id": "R-9d0FlBZGFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "poZsodtlYqsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('russian'))\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "em= ['â˜ï¸', 'â˜¹', 'â˜ºï¸', 'âœŒ', 'âœ¨', 'â¤', 'â¤ï¸', 'â¤ï¸\\u200dğŸ”¥', 'ğŸŒ¸', 'ğŸŒ¹', 'ğŸŒº', 'ğŸŒ¼', 'ğŸ', 'ğŸ„', 'ğŸˆ\\u200dâ¬›', 'ğŸ¤', 'ğŸ°', 'ğŸ‘Œ', 'ğŸ‘ŒğŸ»', 'ğŸ‘ŒğŸ¼', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¿', 'ğŸ‘ğŸ¼', 'ğŸ‘', 'ğŸ‘¾', 'ğŸ’„', 'ğŸ’‹', 'ğŸ’', 'ğŸ’“', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’˜', 'ğŸ’™', 'ğŸ’›', 'ğŸ’œ', 'ğŸ’', 'ğŸ’£', 'ğŸ’¥', 'ğŸ’©', 'ğŸ’ª', 'ğŸ’«', 'ğŸ’¯', 'ğŸ”¥', 'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜…', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜’', 'ğŸ˜”', 'ğŸ˜˜', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ˜£', 'ğŸ˜­', 'ğŸ˜¼', 'ğŸ˜¿', 'ğŸ™‚', 'ğŸ™ˆ', 'ğŸ™Œ', 'ğŸ™ŒğŸ»', 'ğŸ™', 'ğŸ™', 'ğŸ›', 'ğŸ¤', 'ğŸ¤”', 'ğŸ¤—', 'ğŸ¤™', 'ğŸ¤£', 'ğŸ¤¤', 'ğŸ¤¦\\u200dâ™€ï¸', 'ğŸ¤©', 'ğŸ¤ª', 'ğŸ¤¬', 'ğŸ¤®', 'ğŸ¤·\\u200dâ™€ï¸', 'ğŸ¥°', 'ğŸ¥²', 'ğŸ¥³', 'ğŸ¥¶', 'ğŸ¥º', 'ğŸª”', 'ğŸ«’', '\\U0001fae7', '\\U0001faf6ğŸ»']\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ''.join([char for char in text if char not in em])\n",
        "    tokens = text.split()\n",
        "    tokens = [morph.parse(token)[0].normal_form for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['text_preproc'] = df['text'].apply(preprocess_text)\n",
        "text = ' '.join(df[df['label']==0].text_preproc)"
      ],
      "metadata": {
        "id": "D_bgmZLERSpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spell = SpellChecker(language='ru')\n",
        "#  # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ»ĞµĞ¼Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°\n",
        "# morph = pymorphy2.MorphAnalyzer(lang='ru')\n",
        "\n",
        "def generate_feat(x):\n",
        "# Define set of stop words\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "    # Function to count word frequencies\n",
        "    def count_word_frequencies(text):\n",
        "        words = text.split()\n",
        "        return Counter(words)\n",
        "\n",
        "    # Function to count words in a sentence\n",
        "    def count_words(sentence):\n",
        "        return len(sentence.split())\n",
        "\n",
        "    # Function to check if a text contains a certain word\n",
        "    def contains_word(text, word):\n",
        "        return int(word in text.lower())\n",
        "\n",
        "    # Function to count character frequencies\n",
        "    def count_char_frequencies(text):\n",
        "        chars = list(text)\n",
        "        return Counter(chars)\n",
        "\n",
        "    # Function to check if a text contains a certain character\n",
        "    def contains_char(text, char):\n",
        "        return int(char in text)\n",
        "    \n",
        "    def punct(text):\n",
        "        punctuation = string.punctuation.replace(\"!\", \"\").replace(\".\", \"\").replace(\"?\", \"\")\n",
        "        for char in text:\n",
        "          if char in punctuation:\n",
        "              return True\n",
        "        return False\n",
        "    \n",
        "    def count_spelling_errors(sentence):\n",
        "\n",
        "      sentence = re.sub(r'[^\\w\\s-]', '', sentence)\n",
        "      words = sentence.split()\n",
        "      error_count = 0\n",
        "\n",
        "      for word in words:\n",
        "\n",
        "        if type(word)==str:\n",
        "          corrected_word = spell.correction(word)\n",
        "          if word != corrected_word:\n",
        "            error_count += 1\n",
        "      # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº\n",
        "      return error_count\n",
        "\n",
        "\n",
        "    def pos_tag(x):\n",
        "      words = x.text.split()\n",
        "      pos_tags = []\n",
        "      part_of_speech_list = []\n",
        "      case_s_list = []\n",
        "      forms_of_plurality_list =[]\n",
        "      genus_category_list=[]\n",
        "      for word in words:\n",
        "          word = word.lower().strip('.,!?-()[]{}:;\\'\"')\n",
        "          parsed_word = morph.parse(word)[0]\n",
        "\n",
        "          part_of_speech_list.append(parsed_word.tag.POS)\n",
        "          case_s_list.append(parsed_word.tag.case)\n",
        "          forms_of_plurality_list.append(parsed_word.tag.number)\n",
        "          genus_category_list.append(parsed_word.tag.gender)\n",
        "\n",
        "      part_of_speech_count = collections.Counter(part_of_speech_list)\n",
        "      case_s_count = collections.Counter(case_s_list)\n",
        "      forms_of_plurality_count = collections.Counter(forms_of_plurality_list)\n",
        "      genus_category_count = collections.Counter(genus_category_list)\n",
        "\n",
        "      dict_list = {\"part_of_speech\":part_of_speech_count,\"case_s\":case_s_count,\"forms_of_plurality\":forms_of_plurality_count,\"genus_category\":genus_category_count}\n",
        "\n",
        "      for i in list(pred_obr_text.keys()):\n",
        "        for n in pred_obr_text[i]:\n",
        "          if n in dict_list[i].keys():\n",
        "            df[n]= dict_list[i][n]\n",
        "          else:\n",
        "            df[n]= 0\n",
        "      return df\n",
        "\n",
        "\n",
        "    # Generate features\n",
        "    df['contains_,'] = df['text'].progress_apply(lambda x: int(\" ,\" in x))\n",
        "    df['contains_double_space'] = df['text'].progress_apply(lambda x: int(\"  \" in x))\n",
        "    df['contains_punctuation'] = df['text'].progress_apply(lambda x: int(punct(x)))\n",
        "    df['text_length'] = df['text'].progress_apply(len)\n",
        "    df['num_sentences'] = df['text'].progress_apply(lambda x: len(nltk.sent_tokenize(x)))\n",
        "    df['num_words'] = df['text'].progress_apply(lambda x: len(nltk.word_tokenize(x)))\n",
        "    df['count_error'] = df['text'].progress_apply(lambda x: count_spelling_errors(x))\n",
        "    df['percent_error_word'] = df['count_error'] / df['num_words']\n",
        "    df['percent_error_sentence_true'] = df['count_error'] / df['num_sentences']\n",
        "    df['num_stopwords'] = df['text'].progress_apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
        "    df['percent_stopwords'] = df['num_stopwords'] / df['num_words']\n",
        "    df['num_punctuation'] = df['text'].progress_apply(lambda x: len([char for char in x if char in string.punctuation]))\n",
        "    df['num_unique_words'] = df['text'].progress_apply(lambda x: len(set(x.split())))\n",
        "    df['num_uppercase'] = df['text'].progress_apply(lambda x: len([char for char in x if char.isupper()]))\n",
        "    df['num_exclamation_marks'] = df['text'].progress_apply(lambda x: x.count('!'))\n",
        "    df['num_question_marks'] = df['text'].progress_apply(lambda x: x.count('?'))\n",
        "    df['avg_word_length'] = df['text'].progress_apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
        "    df['avg_words_per_sentence'] = df['text'].progress_apply(lambda x: sum(count_words(sentence) for sentence in nltk.sent_tokenize(x)) / len(nltk.sent_tokenize(x)))\n",
        "    df['contains_@'] = df['text'].progress_apply(lambda x: contains_char(x, '@'))\n",
        "    df['contains_#'] = df['text'].progress_apply(lambda x: contains_char(x, '#'))\n",
        "    return df "
      ],
      "metadata": {
        "id": "AGlctizxB0JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_obr_text = {'part_of_speech':['NOUN','ADJF','ADJS','COMP','VERB','INFN','PRTF','PRTS','GRND','NUMR','ADVB','NPRO','PRED','PREP','CONJ','PRCL','INTJ'],\n",
        "                 'case_s':['nomn','gent','datv','accs','ablt','loct','voct','gen2','acc2','loc2'],\n",
        "                 'forms_of_plurality':['sing','plur'],\n",
        "                 'genus_category':['masc','femn','neut']}\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "def pos_tagging(x):\n",
        "\n",
        "    words = x.text.split()\n",
        "    pos_tags = []\n",
        "    part_of_speech_list = []\n",
        "    case_s_list = []\n",
        "    forms_of_plurality_list =[]\n",
        "    genus_category_list=[]\n",
        "    for word in words:\n",
        "        word = word.lower().strip('.,!?-()[]{}:;\\'\"')\n",
        "        parsed_word = morph.parse(word)[0]\n",
        "\n",
        "        part_of_speech_list.append(parsed_word.tag.POS)\n",
        "        case_s_list.append(parsed_word.tag.case)\n",
        "        forms_of_plurality_list.append(parsed_word.tag.number)\n",
        "        genus_category_list.append(parsed_word.tag.gender)\n",
        "\n",
        "    part_of_speech_count = collections.Counter(part_of_speech_list)\n",
        "    case_s_count = collections.Counter(case_s_list)\n",
        "    forms_of_plurality_count = collections.Counter(forms_of_plurality_list)\n",
        "    genus_category_count = collections.Counter(genus_category_list)\n",
        "\n",
        "    dict_list = {\"part_of_speech\":part_of_speech_count,\"case_s\":case_s_count,\"forms_of_plurality\":forms_of_plurality_count,\"genus_category\":genus_category_count}\n",
        "    return dict_list"
      ],
      "metadata": {
        "id": "rszOCgiqfZup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_dict = {\n",
        "    \"positive\": [\"â˜ºï¸\", \"âœ¨\", \"â¤\", \"â¤ï¸\", \"â¤ï¸\\u200dğŸ”¥\", \"ğŸŒ¸\", \"ğŸŒ¹\", \"ğŸŒº\", \"ğŸŒ¼\", \"ğŸ\", \"ğŸ„\", \"ğŸ¤\", \"ğŸ°\", \"ğŸ‘Œ\", \"ğŸ‘ŒğŸ»\", \"ğŸ‘ŒğŸ¼\", \"ğŸ‘\", \"ğŸ‘ğŸ»\", \"ğŸ‘ğŸ¼\", \"ğŸ‘ğŸ½\", \"ğŸ‘ğŸ¿\", \"ğŸ‘\", \"ğŸ’„\", \"ğŸ’‹\", \"ğŸ’\", \"ğŸ’“\", \"ğŸ’•\", \"ğŸ’–\", \"ğŸ’—\", \"ğŸ’˜\", \"ğŸ’™\", \"ğŸ’›\", \"ğŸ’œ\", \"ğŸ’\", \"ğŸ’«\", \"ğŸ’¯\", \"ğŸ”¥\", \"ğŸ˜€\", \"ğŸ˜\", \"ğŸ˜‚\", \"ğŸ˜ƒ\", \"ğŸ˜…\", \"ğŸ˜‰\", \"ğŸ˜Š\", \"ğŸ˜‹\", \"ğŸ˜Œ\", \"ğŸ˜\", \"ğŸ˜˜\", \"ğŸ˜œ\", \"ğŸ™‚\", \"ğŸ™Œ\", \"ğŸ™ŒğŸ»\", \"ğŸ™\", \"ğŸ›\", \"ğŸ¤\", \"ğŸ¤—\", \"ğŸ¤™\", \"ğŸ¤£\", \"ğŸ¤©\", \"ğŸ¤ª\", \"ğŸ¥°\", \"ğŸ¥³\", \"ğŸª”\", \"ğŸ«’\", \"\\U0001fae7\", \"\\U0001faf6ğŸ»\"],\n",
        "    \"neutral\": [\"â˜ï¸\", \"âœŒ\", \"ğŸˆ\\u200dâ¬›\", \"ğŸ‘¾\", \"ğŸ’£\", \"ğŸ’¥\", \"ğŸ’©\", \"ğŸ’ª\", \"ğŸ˜‰\", \"ğŸ˜\", \"ğŸ˜¼\", \"ğŸ™ˆ\", \"ğŸ™\", \"ğŸ¤”\", \"ğŸ¥²\", \"ğŸ¥¶\", \"ğŸ¥º\"],\n",
        "    \"negative\": [\"â˜¹\", \"ğŸ˜’\", \"ğŸ˜”\", \"ğŸ˜\", \"ğŸ˜¡\", \"ğŸ˜¢\", \"ğŸ˜£\", \"ğŸ˜­\", \"ğŸ˜¿\", \"ğŸ¤¤\", \"ğŸ¤¦\\u200dâ™€ï¸\", \"ğŸ¤¬\", \"ğŸ¤®\", \"ğŸ¤·\\u200dâ™€ï¸\"]\n",
        "}\n",
        "\n",
        "\n",
        "def generate_feat_emoji_ton(x):\n",
        "  \n",
        "    def tone(x):\n",
        "      pridicted =[0,0,0]\n",
        "      if type(x)==str:\n",
        "        inputs = tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        predicted = torch.nn.functional.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
        "      return list(predicted)\n",
        "    \n",
        "    def emo_sentiment(x):\n",
        "      em = [e['emoji'] for e in emoji.emoji_list(x)]\n",
        "      emojes=['positive','neutral','negative']\n",
        "      for i in em:\n",
        "        for key, val in emoji_dict.items():\n",
        "          if i in val:\n",
        "            emojes.append(key)\n",
        "      dicts = collections.Counter(emojes)\n",
        "      new_dict = {key: value - 1 for key, value in dicts.items()}\n",
        "      return new_dict\n",
        "\n",
        "\n",
        "    def has_multiple_exclamation_marks(text):\n",
        "      return bool(re.search(r'!!+', text))\n",
        "\n",
        "    b = df.progress_apply(lambda x: pos_tagging(x),axis=1)\n",
        "\n",
        "    for i in pred_obr_text.keys():\n",
        "      for n in pred_obr_text[i]:\n",
        "        j = []\n",
        "        for m in range(len(b)):\n",
        "          if n in b[m][i].keys():\n",
        "            j.append(b[m][i][n])\n",
        "          else:\n",
        "            j.append(b[m][i][n])\n",
        "        df[n]=j\n",
        "    \n",
        "    df['count_emoji'] = df['text'].progress_apply(lambda x: len(emoji.distinct_emoji_list(x)))\n",
        "    df['emoji_per_word'] = df['count_emoji'] / df['num_words']\n",
        "    df['emoji_per_sentence'] = df['count_emoji'] / df['num_sentences']\n",
        "    df['contains_!_and_more'] = df['text'].progress_apply(lambda x: int(has_multiple_exclamation_marks(x)))\n",
        "    df[['neutral','positive','negative']] = df['text'].progress_apply(lambda x: pd.Series(tone(x)))\n",
        "    df[['positive_emoj','neutral_emoj','negative_emoj']] = df['text'].progress_apply(lambda x: pd.Series(emo_sentiment(x)))\n",
        "    \n",
        "    return df "
      ],
      "metadata": {
        "id": "5B6yXxsjf9mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = (df.pipe(generate_feat)\n",
        "            .pipe(generate_feat_emoji_ton))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D_SPpLfh6hG",
        "outputId": "2c498667-2da1-4dcf-a989-c6baea3b1954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 197873.22it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 237991.45it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 101940.24it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 273034.11it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 10732.78it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:01<00:00, 2624.58it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [50:43<00:00,  1.03it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 151720.76it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 105003.02it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 208925.79it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 115089.63it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 445537.80it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 576481.44it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 153105.77it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 8896.62it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 451823.37it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 269611.63it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:10<00:00, 297.89it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 20673.47it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:00<00:00, 193543.22it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [07:06<00:00,  7.32it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3123/3123 [00:01<00:00, 1714.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pipeline.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWl0n0kNB75c",
        "outputId": "2d820060-3b85-4013-d68e-766a706e6acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7y0yVPQqEkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}