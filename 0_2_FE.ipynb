{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Q7Ap_Gq2TMqKs_rTwCINUCd0Y7vXTnrh",
      "authorship_tag": "ABX9TyPV6EzLyX7P3DoHRZtqb95d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dicere/WB_Internship/blob/main/0_2_FE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXhACCW3Xdih"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install pymorphy2\n",
        "!pip string\n",
        "!pip install emoji\n",
        "!pip install catboost\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install pyspellchecker\n",
        "!pip install deeppavlov\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict"
      ],
      "metadata": {
        "id": "m3Eu1jLuPTSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import pymorphy2\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import emoji\n",
        "import collections\n",
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from deeppavlov import build_model, configs\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import BertTokenizerFast\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n_XM5iUeXoU4",
        "outputId": "ec14cd41-347b-49d4-d49c-f61483950c00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from catboost import CatBoostClassifier\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from lazypredict.Supervised import LazyClassifier"
      ],
      "metadata": {
        "id": "dbWZK5pJXp6x"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score, classification_report, recall_score, precision_recall_curve,precision_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "S9qWifcZP1eP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment-rusentiment')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment-rusentiment', return_dict=True)\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "tlPtjZ2SdYaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/WB_стажировка/wb_school_task_2.csv.gzip',compression='gzip')\n",
        "df = df.drop_duplicates(ignore_index=True)"
      ],
      "metadata": {
        "id": "R-9d0FlBZGFQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('russian'))\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "em= ['☝️', '☹', '☺️', '✌', '✨', '❤', '❤️', '❤️\\u200d🔥', '🌸', '🌹', '🌺', '🌼', '🎁', '🎄', '🐈\\u200d⬛', '🐤', '🐰', '👌', '👌🏻', '👌🏼', '👍', '👍🏻', '👍🏼', '👍🏽', '👍🏿', '👎🏼', '👏', '👾', '💄', '💋', '💐', '💓', '💕', '💖', '💗', '💘', '💙', '💛', '💜', '💞', '💣', '💥', '💩', '💪', '💫', '💯', '🔥', '😀', '😁', '😂', '😃', '😅', '😉', '😊', '😋', '😌', '😍', '😏', '😒', '😔', '😘', '😜', '😞', '😡', '😢', '😣', '😭', '😼', '😿', '🙂', '🙈', '🙌', '🙌🏻', '🙍', '🙏', '🛁', '🤍', '🤔', '🤗', '🤙', '🤣', '🤤', '🤦\\u200d♀️', '🤩', '🤪', '🤬', '🤮', '🤷\\u200d♀️', '🥰', '🥲', '🥳', '🥶', '🥺', '🪔', '🫒', '\\U0001fae7', '\\U0001faf6🏻']\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ''.join([char for char in text if char not in em])\n",
        "    tokens = text.split()\n",
        "    tokens = [morph.parse(token)[0].normal_form for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['text_preproc'] = df['text'].apply(preprocess_text)\n",
        "text = ' '.join(df[df['label']==0].text_preproc)"
      ],
      "metadata": {
        "id": "D_bgmZLERSpf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_obr_text = {'part_of_speech':['NOUN','ADJF','ADJS','COMP','VERB','INFN','PRTF','PRTS','GRND','NUMR','ADVB','NPRO','PRED','PREP','CONJ','PRCL','INTJ'],\n",
        "                 'case_s':['nomn','gent','datv','accs','ablt','loct','voct','gen2','acc2','loc2'],\n",
        "                 'forms_of_plurality':['sing','plur'],\n",
        "                 'genus_category':['masc','femn','neut']}\n",
        "spell = SpellChecker(language='ru')\n",
        "#  # Инициализация объекта лемматизатора\n",
        "# morph = pymorphy2.MorphAnalyzer(lang='ru')\n",
        "\n",
        "def generate_feat(x):\n",
        "# Define set of stop words\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "    # Function to count word frequencies\n",
        "    def count_word_frequencies(text):\n",
        "        words = text.split()\n",
        "        return Counter(words)\n",
        "\n",
        "    # Function to count words in a sentence\n",
        "    def count_words(sentence):\n",
        "        return len(sentence.split())\n",
        "\n",
        "    # Function to check if a text contains a certain word\n",
        "    def contains_word(text, word):\n",
        "        return int(word in text.lower())\n",
        "\n",
        "    # Function to count character frequencies\n",
        "    def count_char_frequencies(text):\n",
        "        chars = list(text)\n",
        "        return Counter(chars)\n",
        "\n",
        "    # Function to check if a text contains a certain character\n",
        "    def contains_char(text, char):\n",
        "        return int(char in text)\n",
        "    \n",
        "    def punct(text):\n",
        "        punctuation = string.punctuation.replace(\"!\", \"\").replace(\".\", \"\").replace(\"?\", \"\")\n",
        "        for char in text:\n",
        "          if char in punctuation:\n",
        "              return True\n",
        "        return False\n",
        "    \n",
        "    def count_spelling_errors(sentence):\n",
        "\n",
        "      sentence = re.sub(r'[^\\w\\s-]', '', sentence)\n",
        "      words = sentence.split()\n",
        "      error_count = 0\n",
        "\n",
        "      for word in words:\n",
        "\n",
        "        if type(word)==str:\n",
        "          corrected_word = spell.correction(word)\n",
        "          if word != corrected_word:\n",
        "            error_count += 1\n",
        "      # Возвращаем количество ошибок\n",
        "      return error_count\n",
        "\n",
        "\n",
        "    def pos_tag(x):\n",
        "      words = x.text.split()\n",
        "      pos_tags = []\n",
        "      part_of_speech_list = []\n",
        "      case_s_list = []\n",
        "      forms_of_plurality_list =[]\n",
        "      genus_category_list=[]\n",
        "      for word in words:\n",
        "          word = word.lower().strip('.,!?-()[]{}:;\\'\"')\n",
        "          parsed_word = morph.parse(word)[0]\n",
        "\n",
        "          part_of_speech_list.append(parsed_word.tag.POS)\n",
        "          case_s_list.append(parsed_word.tag.case)\n",
        "          forms_of_plurality_list.append(parsed_word.tag.number)\n",
        "          genus_category_list.append(parsed_word.tag.gender)\n",
        "\n",
        "      part_of_speech_count = collections.Counter(part_of_speech_list)\n",
        "      case_s_count = collections.Counter(case_s_list)\n",
        "      forms_of_plurality_count = collections.Counter(forms_of_plurality_list)\n",
        "      genus_category_count = collections.Counter(genus_category_list)\n",
        "\n",
        "      dict_list = {\"part_of_speech\":part_of_speech_count,\"case_s\":case_s_count,\"forms_of_plurality\":forms_of_plurality_count,\"genus_category\":genus_category_count}\n",
        "\n",
        "      for i in list(pred_obr_text.keys()):\n",
        "        for n in pred_obr_text[i]:\n",
        "          if n in dict_list[i].keys():\n",
        "            df[n]= dict_list[i][n]\n",
        "          else:\n",
        "            df[n]= 0\n",
        "      return df\n",
        "\n",
        "\n",
        "    # Generate features\n",
        "    df['contains_,'] = df['text'].progress_apply(lambda x: int(\" ,\" in x))\n",
        "    df['contains_double_space'] = df['text'].progress_apply(lambda x: int(\"  \" in x))\n",
        "    df['contains_punctuation'] = df['text'].progress_apply(lambda x: int(punct(x)))\n",
        "    df['text_length'] = df['text'].progress_apply(len)\n",
        "    df['num_sentences'] = df['text'].progress_apply(lambda x: len(nltk.sent_tokenize(x)))\n",
        "    df['num_words'] = df['text'].progress_apply(lambda x: len(nltk.word_tokenize(x)))\n",
        "    df['count_error'] = df['text'].progress_apply(lambda x: count_spelling_errors(x))\n",
        "    df['percent_error_word'] = df['count_error'] / df['num_words']\n",
        "    df['percent_error_sentence_true'] = df['count_error'] / df['num_sentences']\n",
        "    df['num_stopwords'] = df['text'].progress_apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
        "    df['percent_stopwords'] = df['num_stopwords'] / df['num_words']\n",
        "    df['num_punctuation'] = df['text'].progress_apply(lambda x: len([char for char in x if char in string.punctuation]))\n",
        "    df['num_unique_words'] = df['text'].progress_apply(lambda x: len(set(x.split())))\n",
        "    df['num_uppercase'] = df['text'].progress_apply(lambda x: len([char for char in x if char.isupper()]))\n",
        "    df['num_exclamation_marks'] = df['text'].progress_apply(lambda x: x.count('!'))\n",
        "    df['num_question_marks'] = df['text'].progress_apply(lambda x: x.count('?'))\n",
        "    df['avg_word_length'] = df['text'].progress_apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
        "    df['avg_words_per_sentence'] = df['text'].progress_apply(lambda x: sum(count_words(sentence) for sentence in nltk.sent_tokenize(x)) / len(nltk.sent_tokenize(x)))\n",
        "    df['contains_@'] = df['text'].progress_apply(lambda x: contains_char(x, '@'))\n",
        "    df['contains_#'] = df['text'].progress_apply(lambda x: contains_char(x, '#'))\n",
        "    b = df.progress_apply(lambda x: pos_tagging(x),axis=1)\n",
        "    return df "
      ],
      "metadata": {
        "id": "AGlctizxB0JH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_obr_text = {'part_of_speech':['NOUN','ADJF','ADJS','COMP','VERB','INFN','PRTF','PRTS','GRND','NUMR','ADVB','NPRO','PRED','PREP','CONJ','PRCL','INTJ'],\n",
        "                 'case_s':['nomn','gent','datv','accs','ablt','loct','voct','gen2','acc2','loc2'],\n",
        "                 'forms_of_plurality':['sing','plur'],\n",
        "                 'genus_category':['masc','femn','neut']}\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "def pos_tagging(x):\n",
        "    words = x.text.split()\n",
        "    pos_tags = []\n",
        "    part_of_speech_list = []\n",
        "    case_s_list = []\n",
        "    forms_of_plurality_list =[]\n",
        "    genus_category_list=[]\n",
        "    for word in words:\n",
        "        word = word.lower().strip('.,!?-()[]{}:;\\'\"')\n",
        "        parsed_word = morph.parse(word)[0]\n",
        "\n",
        "        part_of_speech_list.append(parsed_word.tag.POS)\n",
        "        case_s_list.append(parsed_word.tag.case)\n",
        "        forms_of_plurality_list.append(parsed_word.tag.number)\n",
        "        genus_category_list.append(parsed_word.tag.gender)\n",
        "\n",
        "    part_of_speech_count = collections.Counter(part_of_speech_list)\n",
        "    case_s_count = collections.Counter(case_s_list)\n",
        "    forms_of_plurality_count = collections.Counter(forms_of_plurality_list)\n",
        "    genus_category_count = collections.Counter(genus_category_list)\n",
        "    \n",
        "    dict_list = {\"part_of_speech\":part_of_speech_count,\"case_s\":case_s_count,\"forms_of_plurality\":forms_of_plurality_count,\"genus_category\":genus_category_count}\n",
        "    return dict_list"
      ],
      "metadata": {
        "id": "rszOCgiqfZup"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_dict = {\n",
        "    \"positive\": [\"☺️\", \"✨\", \"❤\", \"❤️\", \"❤️\\u200d🔥\", \"🌸\", \"🌹\", \"🌺\", \"🌼\", \"🎁\", \"🎄\", \"🐤\", \"🐰\", \"👌\", \"👌🏻\", \"👌🏼\", \"👍\", \"👍🏻\", \"👍🏼\", \"👍🏽\", \"👍🏿\", \"👏\", \"💄\", \"💋\", \"💐\", \"💓\", \"💕\", \"💖\", \"💗\", \"💘\", \"💙\", \"💛\", \"💜\", \"💞\", \"💫\", \"💯\", \"🔥\", \"😀\", \"😁\", \"😂\", \"😃\", \"😅\", \"😉\", \"😊\", \"😋\", \"😌\", \"😍\", \"😘\", \"😜\", \"🙂\", \"🙌\", \"🙌🏻\", \"🙏\", \"🛁\", \"🤍\", \"🤗\", \"🤙\", \"🤣\", \"🤩\", \"🤪\", \"🥰\", \"🥳\", \"🪔\", \"🫒\", \"\\U0001fae7\", \"\\U0001faf6🏻\"],\n",
        "    \"neutral\": [\"☝️\", \"✌\", \"🐈\\u200d⬛\", \"👾\", \"💣\", \"💥\", \"💩\", \"💪\", \"😉\", \"😏\", \"😼\", \"🙈\", \"🙍\", \"🤔\", \"🥲\", \"🥶\", \"🥺\"],\n",
        "    \"negative\": [\"☹\", \"😒\", \"😔\", \"😞\", \"😡\", \"😢\", \"😣\", \"😭\", \"😿\", \"🤤\", \"🤦\\u200d♀️\", \"🤬\", \"🤮\", \"🤷\\u200d♀️\"]\n",
        "}\n",
        "\n",
        "\n",
        "def generate_feat_emoji_ton(x):\n",
        "  \n",
        "    def tone(x):\n",
        "      pridicted =[0,0,0]\n",
        "      if type(x)==str:\n",
        "        inputs = tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        predicted = torch.nn.functional.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
        "      return list(predicted)\n",
        "    \n",
        "    def emo_sentiment(x):\n",
        "      em = [e['emoji'] for e in emoji.emoji_list(x)]\n",
        "      emojes=['positive','neutral','negative']\n",
        "      for i in em:\n",
        "        for key, val in emoji_dict.items():\n",
        "          if i in val:\n",
        "            emojes.append(key)\n",
        "      dicts = collections.Counter(emojes)\n",
        "      new_dict = {key: value - 1 for key, value in dicts.items()}\n",
        "      return new_dict\n",
        "\n",
        "\n",
        "    def has_multiple_exclamation_marks(text):\n",
        "      return bool(re.search(r'!!+', text))\n",
        "\n",
        "    b = df.progress_apply(lambda x: pos_tagging(x),axis=1)\n",
        "\n",
        "    for i in pred_obr_text.keys():\n",
        "      for n in pred_obr_text[i]:\n",
        "        j = []\n",
        "        for m in range(len(b)):\n",
        "          if n in b[m][i].keys():\n",
        "            j.append(b[m][i][n])\n",
        "          else:\n",
        "            j.append(b[m][i][n])\n",
        "        df[n]=j\n",
        "    df['count_emoji'] = df['text'].progress_apply(lambda x: len(emoji.distinct_emoji_list(x)))\n",
        "    df['emoji_per_word'] = df['count_emoji'] / df['num_words']\n",
        "    df['emoji_per_sentence'] = df['count_emoji'] / df['num_sentences']\n",
        "    df['contains_!_and_more'] = df['text'].progress_apply(lambda x: int(has_multiple_exclamation_marks(x)))\n",
        "    df[['neutral','positive','negative']] = df['text'].progress_apply(lambda x: pd.Series(tone(x)))\n",
        "    df[['positive_emoj','neutral_emoj','negative_emoj']] = df['text'].progress_apply(lambda x: pd.Series(emo_sentiment(x)))\n",
        "    \n",
        "    return df "
      ],
      "metadata": {
        "id": "5B6yXxsjf9mz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = (df.pipe(generate_feat)\n",
        "            .pipe(generate_feat_emoji_ton))"
      ],
      "metadata": {
        "id": "9D_SPpLfh6hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_feat = ['text']\n",
        "feature=pipeline[pipeline.columns[3:]].columns"
      ],
      "metadata": {
        "id": "wzxQkoSyPMqg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pipeline[feature].drop(columns=['label','text','text_preproc'])\n",
        "y = pipeline['label']"
      ],
      "metadata": {
        "id": "wYR4GOBFPk17"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "q_i8ykArPp7y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_precsion_recall(y_test, pred):\n",
        "  precision = precision_score(y_test, pred)\n",
        "  recall = recall_score(y_test, pred)\n",
        "  return [precision, recall]"
      ],
      "metadata": {
        "id": "ofjFK73bPsIr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LazyClassifier(verbose=0,ignore_warnings=False, custom_metric=predict_precsion_recall)\n",
        "models, predictions = clf.fit(X_train[X_train.columns[1:]], X_test[X_test.columns[1:]], y_train, y_test)\n",
        "models[['Precision', 'Recall']] = models['predict_precsion_recall'].apply(lambda x: pd.Series(x))\n",
        "models.drop(columns=['predict_precsion_recall'],inplace=True)\n",
        "models.sort_values(by='F1 Score', ascending=False,inplace=True)\n",
        "models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1018
        },
        "id": "_twmYdpjPvRX",
        "outputId": "2d7f31d8-799f-45cf-da39-5675aa7df2e0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 6/29 [00:08<00:27,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CategoricalNB model failed to execute\n",
            "Negative values in data passed to CategoricalNB (input X)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 26/29 [00:13<00:00,  3.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StackingClassifier model failed to execute\n",
            "StackingClassifier.__init__() missing 1 required positional argument: 'estimators'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:14<00:00,  1.95it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
              "Model                                                                           \n",
              "BaggingClassifier                  0.76               0.59     0.59      0.72   \n",
              "XGBClassifier                      0.74               0.59     0.59      0.71   \n",
              "LGBMClassifier                     0.75               0.59     0.59      0.71   \n",
              "AdaBoostClassifier                 0.74               0.57     0.57      0.70   \n",
              "RandomForestClassifier             0.76               0.56     0.56      0.69   \n",
              "KNeighborsClassifier               0.73               0.56     0.56      0.69   \n",
              "NuSVC                              0.76               0.55     0.55      0.68   \n",
              "LogisticRegression                 0.74               0.55     0.55      0.68   \n",
              "RidgeClassifier                    0.74               0.55     0.55      0.68   \n",
              "LinearSVC                          0.74               0.54     0.54      0.68   \n",
              "RidgeClassifierCV                  0.74               0.54     0.54      0.68   \n",
              "LinearDiscriminantAnalysis         0.73               0.54     0.54      0.67   \n",
              "ExtraTreesClassifier               0.75               0.54     0.54      0.67   \n",
              "DecisionTreeClassifier             0.65               0.56     0.56      0.66   \n",
              "LabelPropagation                   0.68               0.53     0.53      0.65   \n",
              "LabelSpreading                     0.68               0.53     0.53      0.65   \n",
              "SVC                                0.75               0.52     0.52      0.65   \n",
              "ExtraTreeClassifier                0.65               0.55     0.55      0.65   \n",
              "SGDClassifier                      0.66               0.54     0.54      0.65   \n",
              "Perceptron                         0.66               0.53     0.53      0.64   \n",
              "NearestCentroid                    0.63               0.57     0.57      0.64   \n",
              "CalibratedClassifierCV             0.74               0.51     0.51      0.64   \n",
              "BernoulliNB                        0.63               0.57     0.57      0.64   \n",
              "PassiveAggressiveClassifier        0.63               0.52     0.52      0.63   \n",
              "DummyClassifier                    0.74               0.50     0.50      0.62   \n",
              "GaussianNB                         0.27               0.50     0.50      0.12   \n",
              "QuadraticDiscriminantAnalysis      0.26               0.50     0.50      0.11   \n",
              "\n",
              "                               Time Taken  Precision  Recall  \n",
              "Model                                                         \n",
              "BaggingClassifier                    1.32       0.62    0.24  \n",
              "XGBClassifier                        1.27       0.52    0.28  \n",
              "LGBMClassifier                       0.35       0.55    0.25  \n",
              "AdaBoostClassifier                   1.25       0.50    0.22  \n",
              "RandomForestClassifier               0.79       0.71    0.15  \n",
              "KNeighborsClassifier                 0.09       0.46    0.20  \n",
              "NuSVC                                0.55       0.81    0.10  \n",
              "LogisticRegression                   0.10       0.50    0.15  \n",
              "RidgeClassifier                      0.05       0.51    0.14  \n",
              "LinearSVC                            0.71       0.52    0.13  \n",
              "RidgeClassifierCV                    0.07       0.51    0.13  \n",
              "LinearDiscriminantAnalysis           0.14       0.44    0.15  \n",
              "ExtraTreesClassifier                 0.63       0.62    0.10  \n",
              "DecisionTreeClassifier               0.15       0.35    0.36  \n",
              "LabelPropagation                     0.38       0.33    0.22  \n",
              "LabelSpreading                       0.56       0.33    0.22  \n",
              "SVC                                  0.51       0.80    0.05  \n",
              "ExtraTreeClassifier                  0.06       0.33    0.33  \n",
              "SGDClassifier                        0.09       0.32    0.28  \n",
              "Perceptron                           0.03       0.32    0.26  \n",
              "NearestCentroid                      0.03       0.35    0.45  \n",
              "CalibratedClassifierCV               5.31       0.67    0.04  \n",
              "BernoulliNB                          0.08       0.34    0.44  \n",
              "PassiveAggressiveClassifier          0.04       0.29    0.29  \n",
              "DummyClassifier                      0.03       0.00    0.00  \n",
              "GaussianNB                           0.03       0.27    1.00  \n",
              "QuadraticDiscriminantAnalysis        0.06       0.26    1.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60bcf7c8-404d-4dce-a2e8-540f3612a90e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Balanced Accuracy</th>\n",
              "      <th>ROC AUC</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>Time Taken</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>BaggingClassifier</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.32</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBClassifier</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.71</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LGBMClassifier</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdaBoostClassifier</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestClassifier</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNeighborsClassifier</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NuSVC</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LogisticRegression</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RidgeClassifier</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LinearSVC</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RidgeClassifierCV</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LinearDiscriminantAnalysis</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ExtraTreesClassifier</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTreeClassifier</th>\n",
              "      <td>0.65</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LabelPropagation</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LabelSpreading</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVC</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ExtraTreeClassifier</th>\n",
              "      <td>0.65</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SGDClassifier</th>\n",
              "      <td>0.66</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Perceptron</th>\n",
              "      <td>0.66</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NearestCentroid</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CalibratedClassifierCV</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.64</td>\n",
              "      <td>5.31</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BernoulliNB</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PassiveAggressiveClassifier</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DummyClassifier</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GaussianNB</th>\n",
              "      <td>0.27</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QuadraticDiscriminantAnalysis</th>\n",
              "      <td>0.26</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60bcf7c8-404d-4dce-a2e8-540f3612a90e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60bcf7c8-404d-4dce-a2e8-540f3612a90e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60bcf7c8-404d-4dce-a2e8-540f3612a90e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = CatBoostClassifier(verbose=0)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dR9dki1hQArZ",
        "outputId": "d234ede3-170b-40f2-87c4-ac5607cbefc4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7f2ce9a49cc0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "jLdoWvJbd2Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5"
      ],
      "metadata": {
        "id": "1YCDY-A3dt5E"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict_proba( X_test)[:, 1]\n",
        "y_pred_metki = (y_pred> threshold).astype('int')"
      ],
      "metadata": {
        "id": "shMSLNyCdvXd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_metki, average='macro')\n",
        "roc = roc_auc_score(y_test, y_pred)\n",
        "print(\"AUC_ROC----\"+str(roc))\n",
        "print(\"F1-score-----\"+str(f1))\n",
        "print(\"Precision-----\"+str(precision))\n",
        "print(\"Recall-----\"+str(recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7KFu7MJodwrt",
        "outputId": "e7b900b0-82c3-475c-edd9-62422bc96c28"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_ROC----0.7644005270092228\n",
            "F1-score-----0.6110696252044663\n",
            "Precision-----0.7577846681620266\n",
            "Recall-----0.6029644268774703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Бейзлайн без новых фичей\n",
        "##AUC_ROC----0.7663504611330698\n",
        "##F1-score-----0.6477303317854499\n",
        "##Precision-----0.7589015743926597\n",
        "##Recall-----0.6308629776021081"
      ],
      "metadata": {
        "id": "f_ZKm9bwhP-d"
      }
    }
  ]
}